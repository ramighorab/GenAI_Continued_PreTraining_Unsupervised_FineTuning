{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPjkNNaa8EHJkG0yGMCVckH"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "## Installations; to be executed if running this notebook from Google Colab (recommended).\n",
    "#!pip install pymupdf\n",
    "#!pip install bitsandbytes\n",
    "\n"
   ],
   "metadata": {
    "id": "uCOLMDYabN28"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcqxFCLpahBp"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pymupdf\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Try to make pytorch let go of some of its unused allocated memory!\n",
    "#   (to be used if running some cells multiple times in Google Colab)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "metadata": {
    "id": "5IUpFxsGmeqB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "####### CUDA and MPS checks #######\n",
    "\n",
    "# Use this to check if your machine supports CUDA (and would have VRAM)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")\n",
    "print(\"Available memory:\",\n",
    "      torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else \"No CUDA device\")\n",
    "\n",
    "\n",
    "# Use this to check if your machine (e.g. Mac Laptop) supports Metal Performance Shaders\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n"
   ],
   "metadata": {
    "id": "ig5OL82ldpDt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################################\n",
    "########################       1. CONFIGURATION        ####################\n",
    "###########################################################################\n",
    "\n",
    "# --- Model Configuration ---\n",
    "BASE_MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# Other base Models to try:\n",
    "#   - meta-llama/Meta-Llama-3.1-8B-Instruct: a powerful and multilingual model, 8.03B params, Tensor type: BF16\n",
    "#   - mistralai/Mistral-7B-Instruct-v0.3: 7.25B params, Tensor type: BF16\n",
    "#   - microsoft/Phi-3-mini-4k-instruct: Small Language model with 3.82 billion parameters, Tensor Type: BF16\n",
    "#   - google/gemma-2-2b-it: Compact/LightWeight model with 2.61 billion parameter, known for strong performance in multilingual applications. Tensor Type: BF16\n",
    "#   - Qwen/Qwen2.5-7B-Instruct: 7.62B params, Tensor type: BF16\n",
    "#   - openai/gpt-oss-20b: 21.5B params (but they say it works on 16GB RAM machines, Tensor type: BF16Â·U8\n",
    "#   - malhajar/Mistral-7B-v0.1-arabic: A fine-tuned version of Mistral 7B specifically for Arabic.\n",
    "#   - QCRI/Fanar-1-9B-Instruct: A model developed by Qatar's Computing Research Institute, based on a different architecture but highly specialized for Arabic.\n",
    "#   - ALLaM-AI/ALLaM-7B-Instruct-preview: A Saudi-led initiative with a focus on Arabic.\n",
    "#   - Mistral Saba? : 24 billion parameters and is specifically tailored for Arabic language and cultural nuances\n",
    "\n",
    "NEW_MODEL_NAME = (BASE_MODEL_NAME + \"-tuned-for-economics\").replace('/', '-')\n",
    "TASK_TYPE = \"CAUSAL_LM\"  #Causal LM is more suitable for Generative tasks than MLM (Masked Language Modeling - e.g. what BERT uses)\n",
    "\n",
    "# Hugging Face Access Token has to be added in your Secrets tab in Google Colab, with variable name: \"HF_TOKEN\"\n",
    "from google.colab import userdata\n",
    "ACCESS_TOKEN = userdata.get('HF_TOKEN')\n",
    "## Alternatively if running from local machine: store the token in .env file and load it:\n",
    "#from dotenv import load_dotenv\n",
    "# load_dotenv() # Load environment variables\n",
    "# ACCESS_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = ACCESS_TOKEN)\n",
    "\n",
    "# --- Data Configuration ---\n",
    "print(\"Current dir is: \", os.getcwd())\n",
    "PDF_FOLDER_PATH = \"./sample_data/\"  # or ./docs on local machine\n",
    "\n",
    "\n",
    "# --- PEFT (Parameter Efficient Fine-Tuning) & LoRA (Low Rank Adaptation) Configuration ---\n",
    "# These settings configure the LoRA adapters for efficient training.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices. Lower rank means fewer parameters to train.\n",
    "    lora_alpha=32,  # Alpha parameter for scaling.\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers.\n",
    "    bias=\"none\",\n",
    "    task_type=TASK_TYPE,\n",
    "    # Target modules can be specific to the model architecture.\n",
    "    #   For Llama 3, these are common layers to adapt.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "# --- Training Configuration ---\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # A single epoch is often sufficient for domain adaptation.\n",
    "    per_device_train_batch_size=2,  # Batch size per GPU. Adjust based on your VRAM.\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",  # Memory-efficient optimizer.\n",
    "    save_steps=50,  # Save a checkpoint every 50 steps.\n",
    "    logging_steps=10,  # Log training progress every 10 steps.\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    #fp16=True,  # Use 16-bit precision for training.\n",
    "    bf16=True, # Set to True if you have a modern GPU that supports it (e.g., Ampere).\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,  # If > 0, overrides num_train_epochs.\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Free memory at the moment:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))"
   ],
   "metadata": {
    "id": "LWIpn6_YbRbx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################################\n",
    "########################       2. DATA PREP         #######################\n",
    "###########################################################################\n",
    "\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts all text from PDF files in a given folder and combines them.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: the two sample PDF documents used in this project were downloaded from:\n",
    "    #   - UN World Economic Situation and Prospects 2024:\n",
    "    #       https://www.un.org/development/desa/dpad/wp-content/uploads/sites/45/WESP_2024_Web.pdf\n",
    "    #   - UK Parliament - Economic Indicators Report 2025:\n",
    "    #       https://researchbriefings.files.parliament.uk/documents/CBP-9040/CBP-9040.pdf\n",
    "\n",
    "\n",
    "    print(f\"Reading PDFs from: {folder_path}\")\n",
    "    full_text = \"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: Directory not found at {folder_path}\")\n",
    "        print(\"Please put some PDFs in the sample_data folder (or in the docs folder).\")\n",
    "        return None\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {folder_path}.\")\n",
    "        return None\n",
    "\n",
    "    for filename in pdf_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with pymupdf.open(file_path) as doc:\n",
    "                text = \"\".join(page.get_text() for page in doc)\n",
    "                full_text += text + \"\\n\"\n",
    "                print(f\"  - Successfully processed {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read {filename}: {e}\")\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Extract text and create a Hugging Face Dataset\n",
    "corpus_text = extract_text_from_pdfs(PDF_FOLDER_PATH)\n",
    "if corpus_text:\n",
    "    # Create a dataset with a single column named \"text\", which the trainer will use to train the model.\n",
    "    dataset = Dataset.from_dict({\"text\": [corpus_text]})\n",
    "    print(f\"\\nSuccessfully created dataset with {len(corpus_text)} characters.\")\n",
    "else:\n",
    "    print(\"\\nData loading failure!\")\n",
    "    #exit()  #only works if you are executing locally, not on Google Colab.\n",
    "\n",
    "print(\"Free memory at the moment:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))"
   ],
   "metadata": {
    "id": "pRqW1ilHbRfS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################################\n",
    "###################      3.  MODEL LOADING & PEFT SETUP     ###############\n",
    "###########################################################################\n",
    "\n",
    "# --- Load the model with 4-bit quantization (this significantly reduces the memory footprint) ---\n",
    "if (dataset):  # prevent continuation if there were no PDFs or no text was extracted\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    print(\"\\nLoading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" # Automatically maps the model to available devices (GPU/CPU).\n",
    "    )\n",
    "    base_model.config.use_cache = False\n",
    "    base_model.config.pretraining_tp = 1\n",
    "\n",
    "    print (\"Model and its layers:\")\n",
    "    print(base_model)\n",
    "\n",
    "    print(\"Currently used device by base_model: \", base_model.device)\n"
   ],
   "metadata": {
    "id": "quOe0-A3bRik"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if (dataset):\n",
    "    # --- Load the tokenizer ---\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    print(\"Base model and tokenizer loaded successfully.\")\n",
    "\n",
    "    # --- Tokenize the dataset ---\n",
    "    def tokenize_function(examples):\n",
    "        # We just return the tokenized text. The data collator will handle batching and masking.\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    print(\"Free memory at the moment:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if (dataset):\n",
    "    # --- Prepare model for PEFT ---\n",
    "    peft_model = get_peft_model(base_model, lora_config)\n",
    "    print(\"\\nModel prepared for PEFT.\")\n",
    "\n",
    "    print(\"Free memory at the moment:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################################\n",
    "########################       4. TRAINING         ########################\n",
    "###########################################################################\n",
    "\n",
    "print(\"\\nInitializing trainer...\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Use the standard `Trainer` for continued pre-training ---\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "print(\"Free memory at the beginning of training:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "print(\"Free memory after training:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))\n",
    "\n",
    "# --- Save the fine-tuned model ---\n",
    "# This saves the LoRA adapter, not the full model.\n",
    "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
    "print(f\"Fine-tuned model adapter saved to ./{NEW_MODEL_NAME}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "b_RYFryWbRlp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################################\n",
    "###################     5. INFERENCE & COMPARISON       ###################\n",
    "###########################################################################\n",
    "\n",
    "# --- Define a test prompt ---\n",
    "prompt = \"\"\"Based on recent economic trends,\n",
    "what is the outlook for global GDP growth in the coming year and what are the primary risks?\"\"\"\n",
    "\n",
    "\n",
    "# --- Define a generation function ---\n",
    "def generate_response(model, tokenizer, prompt_text):\n",
    "    \"\"\"\n",
    "    Generates a response from a given model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the messages payload for the chat model\n",
    "\n",
    "    # google/gemma-2-2b-it model does not support role \"system\"; it calls it \"assistant\"\n",
    "    system_role = \"assistant\" if BASE_MODEL_NAME==\"google/gemma-2-2b-it\" else \"system\"\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # google/gemma-2-2b-it model is a bit classic and it insists that the user role starts first\n",
    "    if BASE_MODEL_NAME==\"google/gemma-2-2b-it\":\n",
    "        messages.append({\"role\": \"user\", \"content\": \"Please assist me!\"})\n",
    "\n",
    "    messages.append([\n",
    "        {\"role\": system_role, \"content\": \"You are an expert economist. Provide a detailed and insightful analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text},\n",
    "    ])\n",
    "\n",
    "    #print(\"#debugging at generate_response(): messages:\", messages)\n",
    "\n",
    "    print(\"Currently used device by the model: \", model.device)\n",
    "\n",
    "    # Apply the chat template and encode the prompt\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=5000,  # Limit the length of the generated response.\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n"
   ],
   "metadata": {
    "id": "PG0RPAirbRob"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compare the response of the base model to the response of the fine-tuned model\n",
    "\n",
    "##### The main/simple way of doing it\n",
    "print(\"\\n--- Generating response from ORIGINAL BASE MODEL ---\")\n",
    "# For the base model, we don't need to load any adapters.\n",
    "# We can use the 'base_model' object we already have, but we'll set it to evaluation mode.\n",
    "base_model.eval()\n",
    "base_response = generate_response(base_model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response:\\n{base_response}\\n\")\n",
    "\n",
    "\n",
    "##### A potentially alternative way of doing it:\n",
    "# # --- Determine the device ---\n",
    "# # This is mainly for logging; device_map=\"auto\" handles the actual placement.\n",
    "# device_string = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"\\nUsing device: {device_string.upper()}\")\n",
    "#\n",
    "# print(\"\\n--- Generating response from ORIGINAL BASE MODEL ---\")\n",
    "# # Clear memory first\n",
    "# print(\"Free memory before clearing memory:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))\n",
    "# print(\"Trying to clear some memory!\")\n",
    "# del peft_model\n",
    "# del base_model\n",
    "# del trainer\n",
    "# if \"cuda\" in device_string:\n",
    "#     torch.cuda.empty_cache()\n",
    "# elif \"mps\" in device_string:\n",
    "#     torch.mps.empty_cache()\n",
    "\n",
    "# print(\"Free memory after clearing memory:\", torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))\n",
    "#\n",
    "# original_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL_NAME,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "#\n",
    "# original_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "#\n",
    "# base_response = generate_response(original_model, tokenizer, prompt)\n",
    "# print(f\"Prompt: {prompt}\")\n",
    "# print(f\"Response:\\n{base_response}\\n\")\n",
    "\n"
   ],
   "metadata": {
    "id": "cIuZoz-qbRrO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Generating response from NEWLY TRAINED MODEL ---\")\n",
    "# To use our trained model, we load the base model again and then apply the LoRA adapter we just saved.\n",
    "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    offload_folder=\"offload\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "metadata": {
    "id": "vdpu2B7M5MfY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the PEFT model (the LoRA adapter)\n",
    "trained_model = PeftModel.from_pretrained(base_model_for_inference,\n",
    "                                          NEW_MODEL_NAME,\n",
    "                                          is_trainable=False # this tells the PEFT library that we are loading the LoRA adapter only for inference, which allows it to use a more efficient loading method\n",
    "                                          )\n",
    "trained_model.eval()\n",
    "\n"
   ],
   "metadata": {
    "id": "JsXMB47f5oDL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trained_response = generate_response(trained_model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response:\\n{trained_response}\\n\")\n",
    "\n",
    "print(\n",
    "    \"Comparison complete. Observe how the newly trained model's response may be more aligned with the style and content of your training documents.\")\n"
   ],
   "metadata": {
    "id": "37b7iflX5qtq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Mount a Google Drive folder so that the adapted model can be downloaded from Google Colab folders:\n",
    "# #    instructions from here: https://saturncloud.io/blog/how-to-download-multiple-files-or-an-entire-folder-from-google-colab/\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')  # When you get to the authentication, I have to select all things to allow all!\n",
    "\n",
    "# %cd /content/\n",
    "# !zip -r my_model.zip <model-folder-name-here>/\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('my_model.zip')\n"
   ],
   "metadata": {
    "id": "LztjcWDIronY"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
